---
title: "Generalized Linear Models: Binomial models"
output:
  xaringan::moon_reader:
     css: xaringan-themer.css
     nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options: 
  chunk_output_type: console
header-includes :
  - \usepackage {amsmath}
  - \usepackage{mathrsfs}
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#024731",
#  header_font_google = google_font("Josefin Sans"),
#  text_font_google   = google_font("Montserrat", "300", "300i"),
#  code_font_google   = google_font("Fira Mono")
)
```


```{r setup, include=FALSE, echo=F}
library(ggplot2)#plotting functions
library(wesanderson)
theme_set(theme_classic()) # a theme I like.
theme_update(plot.title = element_text(hjust = 0.5), 
             axis.text=element_text(size=15), 
             text=element_text(size=20))
```




class: center, middle, inverse
background-image: url("https://ericjoyner.com/wp-content/uploads/2020/04/Happy-Garden.jpg")
background-position: center

# Categorical data
  
---

# Binomial (logistic) regression 
<br>

We've had 
1. Continuous response, continuous predictor (regression)
2. Continuous response, discrete predictor (t-test, ANOVA)

<br>

Binomial regression has a **discrete response** with continuous or discrete predictors.


<br>

Examples: survival, presence/absence, disease infection/recovery, dilution assays

---
# Logistic regression and the logit link

\begin{align*}
\mathrm{logit}(p) &\equiv \ln\left( \frac{p}{1-p}\right) = \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p\\
p &= \frac{e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p}} \\
Y &\sim \mathrm{binomial}(N,\, p)
\end{align*}

<br>

#### Recall the binomial distribution properties:

\begin{align*}
E(Y) &= Np\\
Var(Y) &= Np(1-p)
\end{align*}



---
#Odds

$\frac{p}{1-p}$ is called the **odds** and gives the relative probability of success.  It is often used in betting. 

.pull-left[
<br>

```{r, echo=F}
p <- c(0.1, 0.25, 0.5, 0.75, 0.9)
odds <- p/(1-p)
log.odds <- log(odds)
knitr::kable(data.frame(p, "odds"=round(odds,2), "logit(log odds)"=round(log.odds,2)), 'html')
```
]

.pull-right[
```{r, out.width="80%", fig.asp=1, echo=F}
xseq <- seq(-5, 5, length.out=1000)
p <- exp(xseq)/(1 + exp(xseq))
ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) + xlab("x") + ylab(expression(e^x/(1+e^x)))
```
]

---
# Interpreting parameters in logistic regression

For a continuous predictor variable, $x_1$ , the regression coefficient, $\beta_1$ , represents the change in log-odds per unit change in $x_1$ holding other predictors constant.

```{r, out.width="90%", fig.asp=0.4, echo=F}
library(gridExtra)
xseq <- seq(-5, 5, length.out=1000)
p <- exp(xseq)/(1 + exp(xseq))
p2 <- exp(-xseq)/(1 + exp(-xseq))
p3 <- exp(0.5*xseq)/(1 + exp(0.5*xseq))

plot1 <- ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.2) +
  geom_line(aes(x=xseq, y=p2), col="cornflowerblue", size=1.2) +
  xlab("x") + ylab("Probability") 

plot2 <- ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.2) +
  geom_line(aes(x=xseq, y=p3), col="cornflowerblue", size=1.2) +
  xlab("x") + ylab("Probability") 

grid.arrange(plot1, plot2, nrow = 1)

```

---
# Interpreting II

The intercept determines the probability at $x=0$: $p=\frac{e^{\beta_0}}{1 + e^{\beta_0}}=?$

```{r, out.width="40%", fig.asp=1/1.62, echo=F, fig.align="center"}
xseq <- seq(-5, 5, length.out=1000)
p <- exp(xseq)/(1 + exp(xseq))
ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) + xlab("x") + ylab("Probability") 
```


---
# Response data in logistic regression

We can have response data of 0 or 1. This can be given to R as factors as well (e.g., `survive`/`die`, `positive`/`negative`, etc.)

```{r, eval=F}
glm.fit  <- glm(y ~ x, data=sim.dat, family=binomial)

```

.pull-left[

```{r, echo=F}
set.seed(1)
x        <- rnorm(20, 0, 10)
y        <- rbinom(length(x), size=1, p=exp(1 + 0.1*x)/(1 + exp(1 + 0.1*x)))
sim.dat  <- data.frame(x=x, y=y)
glm.fit  <- glm(y ~ x, data=sim.dat, family=binomial)

pred.x   <- seq(-30, 30, length.out=100)
pred.sim <- predict(glm.fit, newdata=data.frame(x=pred.x), type="response")
new.dat  <- data.frame(x=pred.x, y=pred.sim)
knitr::kable(sim.dat[1:9,], format="html")
```
]

.pull-right[

```{r, echo=F, out.width="60%"}
ggplot(data=sim.dat, aes(x=x, y=y)) +
  geom_point(size=3) + 
  geom_line(data=new.dat, aes(x=x, y=y)) + 
   xlab("X") + ylab("Y")
```
]

---
# Response data in logistic regression II

... or the response data can be a vector of successes and failures. This is useful if the predictors hold for multiple observations.

```{r, eval=F}
glm(cbind(successes, fails) ~ x, data=sim.dat, family=binomial)
```

.pull-left[
```{r, echo=F}
set.seed(1)
x        <- rnorm(20, 0, 10)
n        <- rpois(20, 20)
y        <- rbinom(length(x), size=n, p=exp(1 - 0.1*x)/(1 + exp(1 - 0.1*x)))
sim.dat  <- data.frame(successes=y, fails=n-y, x=x)
knitr::kable(sim.dat[1:9,], format="html")
```
]

.pull-right[
```{r, echo=F}
glm.fit  <- glm(cbind(successes, fails) ~ x, data=sim.dat, family=binomial)
```

```{r, echo=F, out.width="60%"}
pred.x   <- seq(-30, 30, length.out=100)
pred.sim <- predict(glm.fit, newdata=data.frame(x=pred.x), type="response")
new.dat  <- data.frame(x=pred.x, y=pred.sim)

ggplot(data=sim.dat, aes(x=x, y=successes/(successes+fails))) +
  geom_point(size=3) + 
  geom_line(data=new.dat, aes(x=x, y=y)) + 
  xlab("X") + ylab("Frequency of success")
```
]

---
# Example: Haleakalā silverswords


```{r, echo=F, out.width="22%", fig.align="center"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/4/4f/Argyroxiphium_sandwicense_subsp._macrocephalum.png")
```

  
.footnote[Krushelnycky, P.D., Loope, L.L., Giambelluca, T.W., Starr, F., Starr, K., Drake, D.R., Taylor, A.D. and Robichaux, R.H., 2013. Climate‐associated population declines reverse recovery and threaten future of an iconic high‐elevation plant. Global Change Biology, 19(3)

Image: wikimedia]

---
# Survival

```{r, echo=FALSE, out.width="60%", fig.asp=1/1.62, fig.align="center"}
silver.dat <- read.csv(file="../Data/Silversword.csv")
silver.dat <- silver.dat[order(silver.dat$Elev),]
silver.dat$Survival <- silver.dat$Survive/silver.dat$Pop_size

ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  xlab("Elevation") + ylab('Proportion surviving')
```

---
# Modeling survival with `glm`

```{r, echo=F}
mod.sil <- glm(cbind(Survive, Die) ~ Elev, data=silver.dat, family=binomial)
print(summary(mod.sil))
pred.sil <- predict(mod.sil, newdata=silver.dat, type="response")

```


---
# How did we do?

$R^2_{pseudo}=$ `r 1 - round(794/2074, 2)`

```{r,  out.width="60%", fig.asp=1/1.62, fig.align="center", echo=F}
ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  geom_line(y=pred.sil) + 
  xlab("Elevation") + ylab('Proportion surviving')

```

---
# What about overdispersion?

.pull-left[
\begin{align*}
Y|p &\sim \mathrm{Binomial}(N,\, p)\\
p   &\sim \mathrm{Beta}(\alpha, \beta) \\
Y   &\sim \mathrm{Beta-binomial(N, \mu, \rho)}
\end{align*}
]

.pull-right[
\begin{align*}
E(Y) &= N \mu \\ 
Var(Y) &= N \mu \left(1 - \mu \right) + \rho N(N-1) \mu \left(1 - \mu \right)
\end{align*}
]

```{r, warning=F, message=F, echo=F, out.width="60%", fig.asp=1/1.62, fig.align="center"}

x <- 0:10

plot(x, dbinom(x=x, size=10, prob=0.3), type='l', lwd=2, xlab="y", ylab="Density")
lines(x, rmutil::dbetabinom(y=x, size=10, m=0.3, s=20), lwd=2, col='orange')
lines(x, rmutil::dbetabinom(y=x, size=10, m=0.3, s=5), lwd=2, col='cornflowerblue')


```

---
# Beta-binomial regression

```{r, warning=F, message=F}
library(VGAM) #contains beta-binomial regression
model.bin <- glm(cbind(Survive, Die) ~ Elev, data=silver.dat, family=binomial)
model.bb <- vglm(cbind(Survive, Die) ~ Elev, data=silver.dat, family=betabinomial)

summary(model.bb)
```

---
# Comparing parameter estimates

.pull-left[
**Binomial**
```{r, message=F, warning=F}
confint(model.bin)
```
]
.pull-right[
**Beta-binomial**
```{r}
confint(model.bb)
```
]
---
# Comparing model fits

```{r}
print(c(AIC(model.bin), AIC(model.bb)))
```

```{r, echo=F, out.width="60%", fig.asp=1/1.62, fig.align="center"}
silver.dat$Survival <- silver.dat$Survive/silver.dat$Pop_size #create a vector of the observed survival probabilities
pred.sil.bb <- predict(model.bb, type="response") #this extracts predictions 
ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  geom_line(aes(y=pred.sil.bb), col="cornflowerblue", size=1.2) + 
  xlab("Elevation") + ylab('Proportion surviving')
```

---
#Summary 

- GLM's extend the application of LM's beyond the normal distribution. 
  - Often useful with biological data

- Standard distributions in `glm` cannot handle overdisperison.

- Overdispersion can have strong influences on SE's and p-values
  - Not accounting for overdispersion can lead to overconfidence in estimates and model.
