---
title: "Generalized Linear Models: Count models"
output:
  xaringan::moon_reader:
     css: xaringan-themer.css
     nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options: 
  chunk_output_type: console
header-includes :
  - \usepackage {amsmath}
  - \usepackage{mathrsfs}
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#024731",
#  header_font_google = google_font("Josefin Sans"),
#  text_font_google   = google_font("Montserrat", "300", "300i"),
#  code_font_google   = google_font("Fira Mono")
)
```


```{r setup, include=FALSE, echo=F}
library(ggplot2)#plotting functions
library(wesanderson)
theme_set(theme_classic()) # a theme I like.
theme_update(plot.title = element_text(hjust = 0.5), 
             axis.text=element_text(size=15), 
             text=element_text(size=20))
```


# Goals

<br>

<br>

1. Generalize regression to count and categorical data

2. Identify proper distribution to use for particular datasets

3. Evaluate model quality

  
---
# Linear regression

<br>

### The lm framework: 

\begin{align*}
\mu &= \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p \\
Y &\sim N(\mu,\, \sigma^2)\\
\end{align*}

We assume that the residuals are normally distributed around the mean, $\mu$.

---
# Limitations of `lm`

<br>

Although linear models are a very useful framework, there are some situations where they are not appropriate

- the range of $Y$ is restricted (e.g. binary, count)

-  the variance of $Y$ depends on the mean

<br>

----

Generalized linear models (glms) extend the linear model framework to address both of these issues

---
# Generalized linear regression

In a glm there is some transformation of the mean, $g(\mu)$, called the **link** function, that results in a linear model.

\begin{align*}
g(\mu) &= \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p \\
\mu &= g^{-1}\left(\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p\right) \\
Y &\sim f(\mu,\, \theta)
\end{align*}

We assume that the residuals are distributed around the mean, $\mu$, following some distribution $f(\cdot)$ (e.g., Binomial, Poisson, Negative Binomial). Where $\theta$ is any relevant additional parameters needed to model the  variance. 

<br> 

By choosing an appropriate **link** function, $g( \cdot )$, we will ensure that the mean only takes on values that are supported by the distribution (for example only positive values for the Poisson or between 0 and 1 for the Binomial).

---
class: center, middle, inverse
background-image: url("https://ericjoyner.com/wp-content/uploads/2015/07/Robo-Ramos.jpg")
background-position: center

# Models of count data

---
# Poisson regression

<br>

Use the log-link function: $g(\mu) = ln(\mu)$. Then the inverse link is $e^\cdot$.


\begin{align*}
  \mu &= e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p} \\
  Y &\sim \mathrm{Poisson}(\mu)
\end{align*}

$$ E(Y) = \mu, \,\,\,\,\,\,\,\, Var(Y)=\mu $$

```{r, out.width="35%", fig.asp=0.6, echo=F, fig.align="center"}
xseq <- seq(-5, 5, length.out=1000)
mu <- exp(xseq)
ggplot() +
  geom_line(aes(x=xseq, y=mu), size=1.3) + xlab("x") + ylab(expression(e^x))
```

---
# Including predictor variables

```{r, echo=F, out.width="50%", fig.asp=0.62, fig.align="center"}
set.seed(1)
x<-seq(0,100,1) 
mus<-exp(0.01+0.03*x) # mean at each x
y<-rpois(length(x),lambda= mus) # 1 observation per x
#plot(x, y, pch=16, xlab="% Grass", ylab="Number of Pheasants")
#lines(x, mus)
ggplot(data=data.frame(x, y), aes(x=x, y=y)) + geom_point(size=2) + xlab("% Cover Grass") + ylab("Number of pheasants") 
```


\begin{align*}
  \mu &= e^{0.01 + 0.03\cdot X} \\
  Y &\sim \mathrm{Poisson}(\mu)
\end{align*}

.footnote[simulated data!]
---
# Using `glm`

The syntax for `glm` follows `lm` closely:

```{r, eval=F}
glm(RESPONSE ~ X1 + X2, data=data.csv, family=poisson(link="log"))
```

We've added the `family` argument to specify which distribution to use. We've also added `(link="log")` to specify the link function we use. 


.footnote[Website with a nice table of families and their link functions: https://data.princeton.edu/R/GLMs

---
# `glm` model output

```{r, echo=F}
pheas.dat <- data.frame(grassCover=x, Pheasants=y)
```

```{r}
summary(glm(Pheasants ~ grassCover, data=pheas.dat, family=poisson(link="log")))
```

---
# How to interpret $\beta_0$ and $\beta_1$?
```{r}
coef(glm(Pheasants ~ grassCover, data=pheas.dat, family=poisson))
```

--

----

On the **link** scale parameters mean the same as in `lm`. 
$$\ln(\mu) = \beta_0 + \beta_1 X$$

$\beta_0$: intercept of the log-mean, $\beta_1$: slope of the log-mean

--

-----

On the **natural** scale they may mean something different:

\begin{align*}
  \mu &= e^{\beta_0 + \beta_1 X} \\
  \mu &= e^{\beta_0}\cdot e^{\beta_1 X} 
\end{align*}

$\beta_0$: mean log-fecundity at $X=0$, $\beta_1$: exponential rate of change in fecundity with $X$

---
# Deviance


`    Null deviance: 484.156  on 100  degrees of freedom`

`Residual deviance:  94.911  on  99  degrees of freedom`


<br> 

**Deviance** is a measure of fit for models fit using maximum likelihood.

**Null deviance** is the residual deviance for a model that only contains an intercept vs the saturated model: $=2(\ell (θ_s) − \ell(\theta_0))$

- $\ell (θ_s)$ is the log-likelihood of the *saturated* model with a parameter at each observation 

- $\ell (θ_0)$ is the log-likelihood of a model with intercept only


**Residual deviance** = $2(\ell (θ_s) − \ell(\theta_{model}))$

- $\ell (θ_s)$ is the log-likelihood of the *saturated* model with a parameter at each observation 

- $\ell (θ_{\mathrm{model}})$ is the log-likelihood of the fitted model


---
# Why Deviance?

There is no $R^2$ for glm's typically so we can instead use the proportion deviance explained (called the **pseudo $R^2$**).

$$R^2_\mathrm{pseudo} = 1 - \frac{\mathrm{Residual\,\, deviance}}{\mathrm{Null\,\, deviance}}$$
- **Null deviance** $\approx$ maximum likelihood equivalent of total
sum of squares.

- **Residual deviance** $\approx$ maximum likelihood equivalent of residual
sum of squares.

--

<br>

For the pheasant model, $R^2_{pseudo}=$ `r round(1 - 94.911/484.156,2)` (remember the data are fake!).
---
# Offsets in count models

Count data ( $Y$) are often collected:

- over varying lengths of time
- in sample units that have different areas

<br>

So we are often interested in modeling rates:

\begin{align*}
Y/\mathrm{time}
\end{align*}

or densities:

\begin{align*}
Y/\mathrm{Area}
\end{align*}

---
# Example: MN beaver densities

Each route is a different length so counts will differ as well.

```{r, out.width="40%", echo=F, fig.align="center"}
knitr::include_graphics("../Figures/BeavMap.jpg")
```

---
# Offsets: behind the scenes
Using route length as an **offset** controls for the differential in survey effort. They can be thought of as predictor variables with the coefficient estimate set to 1.

\begin{align*}
ln(\mu) &= \beta_0 + \beta_1\cdot x_1 + ln(offset)\\
\mu &= e^{\beta_0 + \beta_1\cdot x_1 + ln(offset)}\\
\mu &= e^{\beta_0 + \beta_1\cdot x_1} \cdot offset\\
\frac{\mu}{offset} &= e^{\beta_0 + \beta_1\cdot x_1} 
\end{align*}

<br>

Note that we need to apply the link function to the offset for it to be properly included in the model, this is easy to forget!


---
# Beaver density `glm`



```{r, echo=F}
beav.dat <- read.csv(file="../Data/ColCountsFinal.csv")
#beav.dat <- subset(beav.dat, rte.name == "Blackduck" | rte.name == "Kabetogama" | rte.name == "Kanabec" | rte.name == "Cass" | rte.name == "Itasca")
beaver.glm <- glm(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat, family=poisson)

summary(beaver.glm)
```

---
# ANOVA
```{r, fig.align="center", fig.asp=0.62, echo=F}
summary(aov(beaver.glm))
ggplot(data=beav.dat, aes(x=rte.name, y=num.col/rte.km, fill=rte.name)) + geom_boxplot() + theme(axis.text.x=element_blank())  + xlab("Route") 
```

---
# Model selection

### Can use same tools as linear regression. 

<br>

- t-tests on parameter estimates

- Confidence intervals (`confint`)

- ANOVA (f-tests)

- AIC

*but no adjusted $R^2$*!


---
# Is the Poisson appropriate?

<br>

Recall that in a Poisson distribution the mean and variance are equal: $E(Y) = Var(Y)$.

<br>

How can we test this assumption?

- Formal goodness of fit tests exist (Pearson's $\chi^2$)
- Fit model with overdispersion (next section) and compare via AIC

---
# Overdispersion: empirical variance is higher than expected

$Var(Y) > E(Y)$
 
**Reasons** data may be overdispersed

- Omitted variables

- Measurement error

- Wrong distribution


**Consequences** of overdispersion

- Standard errors may underestimated

- More complex models than necessary may be selected


---
# Negative Binomial

The variance in the data is often higher than the Poisson distribution. This is called **overdispersion**. The Negative Binomial distribution can account for overdispersion.
.pull-left[
\begin{align*}
Y|\mu &\sim \mathrm{Poisson}(\mu)\\
\mu &\sim \mathrm{Gamma}(\theta, \, r)\\
Y &\sim \mathrm{Negative\,\, binomial}(\mu, k)
\end{align*}
]
.pull-right[
\begin{align*}
&E(Y) = \mu\\
&Var(Y) = \mu + \frac{\mu^2}{\theta}
\end{align*}
]

```{r, echo=F, eval=T, out.width="50%", fig.asp=1/1.62, fig.align="center"}
x <- 0:12
plot(x, dpois(x=x, lambda=5), type='l',  lwd=2, ylab="Probability")
lines(x, dnbinom(x=x, mu=5, size=2), col="orange", lwd=2)
lines(x, dnbinom(x=x, mu=5, size=10), col="cornflowerblue", lwd=2)
```

---

# Negative binomial regression

```{r}
library(MASS)  #for glm.nb
library(MuMIn) #for AICc

beaver.pois <- glm(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat, family=poisson)
beaver.nb   <- glm.nb(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat)
cat("Poisson: ", AICc(beaver.pois), '\n')
cat("Negative binomial: ", AICc(beaver.nb), '\n')
```

Based on this output is **overdispersion** present?

---
# Comparing confidence intervals

.pull-left[
**Poisson**

```{r}
confint(beaver.pois)[1:3,]
```
]
.pull-right[
**Negative binomial**

```{r}
confint(beaver.nb)[1:3,]
```
]

<br>

----
Not accounting for overdispersion leads to confidence intervals that are too narrow (overconfidence!)

---
# Summary of count models

- Count models can be used to model discrete counts
  - Especially useful when counts are near 0
  - If counts are high, standard linear regression with a transformation tends to work well
  
- Similar tools for hypothesis testing and model selection as standard linear regression

- **Overdispersion** is almost always present. Should default to negative binomial.
  - Not accounting for overdispersion leads to overconfidence.
  
- **Offsets** can be used to control for differential effort in space or time

